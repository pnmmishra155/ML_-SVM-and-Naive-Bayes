{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:  \n",
        "\n",
        "Information Gain (IG) is a metric used in decision trees to measure how well a given feature splits a dataset into target classes. It is based on the concept of entropy, which measures the impurity or randomness in the dataset.\n",
        "\n",
        "Entropy Formula:\n",
        "\n",
        "$Entropy(S)=- \\sum_{i=1}^{c}=pi‚Äãlog2‚Äã(pi‚Äã) $\n",
        "    \n",
        "\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- $ ùëùùëñ$‚Äã is the probability of class ùëñ in dataset $S$\n",
        "\n",
        "- ùëê is the total number of classes\n",
        "\n",
        "- Information Gain Formula:\n",
        "\n",
        "$ IG(S,A)=Entropy(S) - \\sum_{v \\in A} \\frac{|S_v|}{|S|} \\times Entropy(S_v) $\n",
        "\n",
        "**Where:**\n",
        "\n",
        "- ùê¥ Is a feature\n",
        "\n",
        "- ùëÜùë£ is the subset of ùëÜ where feature ùê¥ has value ùë£\n",
        "\n",
        "**Use in Decision Trees:**\n",
        "\n",
        "- At each node, the algorithm calculates IG for all features.\n",
        "\n",
        "- The feature with the highest Information Gain is selected to split the dataset.\n",
        "\n",
        "- This process continues recursively to build the tree\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3mx6sInSeNqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMM7SIwzlxH"
      },
      "source": [
        "##Question 2: What is the difference between Gini Impurity and Entropy? Hint: Directly compares the two main impurity measures, highlighting strengths,weaknesses, and appropriate use cases.\n",
        "\n",
        "Answer:\n",
        "\n",
        "###Definitions / formulas\n",
        "\n",
        "- Gini Impurity (G):\n",
        "\n",
        "$G=1‚àí\\sum_{i=1}^k p_i^2$\n",
        "\n",
        "It measures the probability of misclassifying a randomly chosen sample if it were labeled according to the class distribution in the node.\n",
        "\n",
        "- Entropy (H):\n",
        "\n",
        "$H=‚àí\\sum_{i=1}^k p_i \\log_2 p_i$\n",
        "\n",
        "It measures the expected information (in bits) required to identify the class of a randomly chosen sample.\n",
        "\n",
        "###Range\n",
        "\n",
        "  - Gini: $0 \\leq G \\leq 1‚àí\\frac{1}{k}$ (for binary classes $0 \\leq G \\leq 0.5$).\n",
        "\n",
        "  - Entropy: $0 \\leq H \\leq \\log_2 k$ (for binary classes $0 \\leq H \\leq 1$).\n",
        "\n",
        "###Behavioral comparison\n",
        "\n",
        "  - Both measure impurity; both are 0 when the node is pure.\n",
        "\n",
        "  - Gini is a quadratic function of class probabilities (computationally cheaper ‚Äî no logs).\n",
        "\n",
        "  - Entropy grows more slowly near pure distributions but penalizes mid-range uncertainty more subtly.\n",
        "\n",
        "###When they differ practically\n",
        "\n",
        "  - For many splits they produce similar rankings of candidate splits (i.e., they often choose the same feature), but small differences can appear in edge cases.\n",
        "\n",
        "  - Gini tends to prefer larger, more balanced partitions slightly (because of quadratic form), while entropy is a bit more sensitive to distribution differences.\n",
        "\n",
        "###Computation speed\n",
        "\n",
        "  - Gini is faster (no logarithms). This is why CART (Classification And Regression Trees) uses Gini by default.\n",
        "\n",
        "###Use cases / algorithm defaults\n",
        "\n",
        "**Gini**\n",
        "  - Gini impurity ‚Äî used by CART (and hence scikit-learn‚Äôs DecisionTreeClassifier default). while using the (Classification and Regression Trees) algorithm.\n",
        "\n",
        " - while working with large datasets where speed matters more than slight gains in accuracy.\n",
        "\n",
        "**Entropy**\n",
        "  - Entropy ‚Äî used by ID3/C4.5 family and sometimes chosen when interpreting splits in information-theory terms is desirable.\n",
        "\n",
        "  - while handling datasets where class distribution is highly imbalanced or where accuracy is more important than speed\n",
        "\n",
        "###Strengths and weaknesses\n",
        "\n",
        "  - Gini:\n",
        "\n",
        "    - Faster to compute.\n",
        "\n",
        "    - Often produces shallower trees (practical advantage).\n",
        "\n",
        "    - ‚àí Slight bias towards features with more categories if not controlled.\n",
        "\n",
        "  - Entropy:\n",
        "\n",
        "    - Theoretically grounded in information theory; good for interpreting information gain.\n",
        "\n",
        "    - ‚àí Slightly slower to compute; may produce different splits in some datasets.\n",
        "\n",
        "###Recommendation:\n",
        "\n",
        "Use Gini for performance and typically similar results. Use Entropy when you want information-theoretic interpretability or when experimenting shows entropy gives better validation performance for your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-pruning (also called early stopping) is the practice of halting the growth of a decision tree before it perfectly fits (or grows deep on) the training data, using predefined constraints. The goal is to prevent overfitting by limiting complexity during training.\n",
        "\n",
        "###Common pre-pruning hyperparameters (scikit-learn names included):\n",
        "\n",
        "- max_depth ‚Äî maximum depth of the tree.\n",
        "\n",
        "- min_samples_split ‚Äî minimum number of samples required to split an internal node.\n",
        "\n",
        "- min_samples_leaf ‚Äî minimum number of samples required to be at a leaf node.\n",
        "\n",
        "- max_leaf_nodes ‚Äî maximum number of leaf nodes.\n",
        "\n",
        "- min_impurity_decrease ‚Äî a split will be made only if it decreases impurity by at least this threshold.\n",
        "\n",
        "- ccp_alpha ‚Äî complexity parameter used for Minimal Cost-Complexity Pruning (but note: ccp_alpha is applied in a post-pruning style in scikit-learn via pruning after full growth).\n",
        "\n",
        "###How pre-pruning works in practice\n",
        "\n",
        "- The tree stops splitting further if:\n",
        "\n",
        "  - Maximum depth is reached (max_depth)\n",
        "\n",
        "  - Minimum number of samples to split is not met (min_samples_split)\n",
        "\n",
        "  - Minimum samples in a leaf node is small (min_samples_leaf)\n",
        "\n",
        "  - Information Gain or Gini decrease is below a threshold.\n",
        "\n",
        "- During tree expansion, if a potential split would violate any pre-pruning condition (e.g., would create leaves with fewer than min_samples_leaf), the split is not performed and the node becomes a leaf.\n",
        "\n",
        "- This keeps the tree simpler and reduces variance at the cost of possibly increasing bias.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Faster training time\n",
        "\n",
        "- Produces simpler and more interpretable trees\n",
        "\n",
        "- Controls overfitting by reducing tree complexity.\n",
        "\n",
        "- Less computational time and memory than growing a large tree and post-pruning.\n",
        "\n",
        "- Direct control over model complexity.\n",
        "\n",
        "###Disadvantages\n",
        "\n",
        "- Choosing hyperparameters incorrectly can lead to underfitting.\n",
        "\n",
        "- It might stop growth too early, preventing the discovery of useful, fine-grained patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "vmeg0BYigAxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "jowUC39Gf5kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decision_tree_gini_feature_importance.py\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# 2. Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree with Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42, max_depth=None)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict & evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# 5. Feature importances\n",
        "importances = clf.feature_importances_\n",
        "print(\"\\nFeature importances (feature_name: importance):\")\n",
        "for name, score in zip(feature_names, importances):\n",
        "    print(f\"{name}: {score:.4f}\")\n",
        "\n",
        "# 6. (Optional) Sort and display\n",
        "order = np.argsort(importances)[::-1]\n",
        "print(\"\\nFeatures ranked by importance:\")\n",
        "for idx in order:\n",
        "    print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
        "\n",
        "\"\"\"Interpretation of .feature_importances_\n",
        "Feature importance values show the normalized total reduction of the criterion (Gini impurity) brought by that feature. Larger values ‚Üí more important feature.\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "MVl3sx9jiYBZ",
        "outputId": "1f532605-81a3-455f-d733-f7d90559c99d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.90      0.90      0.90        10\n",
            "   virginica       0.90      0.90      0.90        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.93      0.93      0.93        30\n",
            "weighted avg       0.93      0.93      0.93        30\n",
            "\n",
            "\n",
            "Feature importances (feature_name: importance):\n",
            "sepal length (cm): 0.0062\n",
            "sepal width (cm): 0.0292\n",
            "petal length (cm): 0.5586\n",
            "petal width (cm): 0.4060\n",
            "\n",
            "Features ranked by importance:\n",
            "petal length (cm): 0.5586\n",
            "petal width (cm): 0.4060\n",
            "sepal width (cm): 0.0292\n",
            "sepal length (cm): 0.0062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Interpretation of .feature_importances_\\nFeature importance values show the normalized total reduction of the criterion (Gini impurity) brought by that feature. Larger values ‚Üí more important feature.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc96549"
      },
      "source": [
        "##Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "\n",
        "###Definition.\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used primarily for classification (and can be adapted for regression). It finds an optimal separating hyperplane between classes by maximizing the margin ‚Äî the distance between the hyperplane and the nearest data points from each class (called support vectors).\n",
        "\n",
        "###Key concepts\n",
        "\n",
        "- Hyperplane: In ùëë-dimensional space, a (ùëë‚àí1)-dimensional hyperplane separates the classes.\n",
        "- Margin: Distance between the hyperplane and the closest points. SVM seeks to maximize this margin.\n",
        "- Support Vectors: Points lying closest to the decision boundary; they determine the position of the hyperplane.\n",
        "- Hard-margin vs Soft-margin:\n",
        "  \n",
        "  - Hard-margin SVM requires perfectly separable data.\n",
        "  - Soft-margin SVM introduces slack variables and parameter C to allow some misclassification while controlling regularization.\n",
        "\n",
        "###Objective (primal form, linear separable):\n",
        "\n",
        "- Find $w$ and $b$ to minimize $\\frac{1}{2}‚à•w‚à•^2$ subject to $y_i(w^T x_i + b) \\geq 1$ for all $i$.\n",
        "\n",
        "###Strengths\n",
        "\n",
        "- Effective in high-dimensional spaces.\n",
        "\n",
        "- Uses a subset of training points (support vectors), so memory efficient.\n",
        "\n",
        "- Robust to overfitting with appropriate regularization (C parameter).\n",
        "\n",
        "###Weaknesses\n",
        "\n",
        "- Choice of kernel and hyperparameters critically affects performance.\n",
        "\n",
        "###Scalability:\n",
        "\n",
        "- training time can be high for very large datasets (though linear SVMs and approximations exist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d439d57a"
      },
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "\n",
        "###Definition\n",
        "\n",
        "- The **Kernel Trick** is a method used in Support Vector Machines (SVMs) and other kernelized algorithms. It allows them to implicitly map data into a higher-dimensional feature space without explicitly computing the coordinates of the data in that space. This enables the algorithm to find a linear separating hyperplane in the higher-dimensional space, which corresponds to a non-linear decision boundary in the original lower-dimensional space.\n",
        "\n",
        "###How it works\n",
        "\n",
        "- Instead of transforming the data points $x_i$ and $x_j$ into the higher-dimensional space $\\phi(x_i)$ and $\\phi(x_j)$ and then computing their dot product $\\phi(x_i) \\cdot \\phi(x_j)$, the kernel trick uses a **kernel function** $K(x_i, x_j)$ that directly computes the dot product in the higher-dimensional space:\n",
        "\n",
        "$K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$\n",
        "\n",
        "This avoids the computationally expensive explicit mapping to the high-dimensional space, especially when the dimensionality is very large or even infinite.\n",
        "\n",
        "###Mercer's Theorem\n",
        "\n",
        "- For a function $K(x, x')$ to be a valid kernel, it must produce a positive semi-definite Gram matrix for all finite sets of inputs. This theorem ensures that the kernel function corresponds to an inner product in some feature space, allowing the kernel trick to work.\n",
        "\n",
        "###Common Kernel Functions\n",
        "\n",
        "- **Linear Kernel:** $K(x_i, x_j) = x_i \\cdot x_j$ (This is equivalent to no kernel trick, just a linear SVM)\n",
        "- **Polynomial Kernel:** $K(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d$, where $\\gamma$, $r$, and $d$ are parameters.\n",
        "- **Radial Basis Function (RBF) Kernel:** $K(x_i, x_j) = exp(-\\gamma ||x_i - x_j||^2)$, where $\\gamma$ is a parameter. This is one of the most commonly used kernels.\n",
        "- **Sigmoid Kernel:** $K(x_i, x_j) = tanh(\\gamma x_i \\cdot x_j + r)$, where $\\gamma$ and $r$ are parameters.\n",
        "\n",
        "###Advantages\n",
        "\n",
        "- Allows SVMs to learn non-linear decision boundaries.\n",
        "- Avoids the computational cost of explicitly mapping data to high dimensions.\n",
        "- Enables working with infinite-dimensional feature spaces (e.g., with the RBF kernel).\n",
        "\n",
        "###Disadvantages\n",
        "\n",
        "- Choosing the right kernel and its parameters can be challenging and often requires experimentation (e.g., using cross-validation).\n",
        "- Can still be computationally expensive for very large datasets, even with the kernel trick."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "v0DC0vc8fiVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# svm_wine_compare.py\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling is recommended for SVMs\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Linear SVM\n",
        "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_lin = svm_linear.predict(X_test_scaled)\n",
        "acc_lin = accuracy_score(y_test, y_pred_lin)\n",
        "\n",
        "# RBF SVM (default gamma='scale')\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Linear SVM accuracy: {acc_lin:.4f}\")\n",
        "print(f\"RBF SVM accuracy:    {acc_rbf:.4f}\")\n",
        "print(\"\\nClassification report for RBF SVM:\\n\", classification_report(y_test, y_pred_rbf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZDj5sejiLOY",
        "outputId": "0c58ec2b-6ee9-4848-c907-e32ce4e5eff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM accuracy: 0.9444\n",
            "RBF SVM accuracy:    0.9722\n",
            "\n",
            "Classification report for RBF SVM:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       0.93      1.00      0.97        14\n",
            "           2       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.97        36\n",
            "   macro avg       0.98      0.97      0.97        36\n",
            "weighted avg       0.97      0.97      0.97        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "948efa3f"
      },
      "source": [
        "##Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "Answer:\n",
        "\n",
        "###Na√Øve Bayes (definition)\n",
        "\n",
        "Na√Øve Bayes classifiers are a family of probabilistic classifiers based on Bayes‚Äô theorem with the (na√Øve) assumption that the features are conditionally independent given the target class.it is mainly used for classification tasks\n",
        "\n",
        "###Bayes‚Äô theorem (for classification):\n",
        "\n",
        "$P(y‚à£x) = \\frac{P(x‚à£y)P(y)}{P(x)}$\n",
        "\n",
        "For classification, we seek the class $y$ that maximizes $P(y‚à£x)$. Because $P(x)$ is constant across classes, we use:\n",
        "\n",
        "$\\hat{y} = \\arg \\max_{y} P(y) \\prod_{i=1}^{d} P(x_i‚à£y)$\n",
        "\n",
        "where $x=(x_1,‚Ä¶,x_d)$.\n",
        "\n",
        "###Why ‚Äúna√Øve‚Äù?\n",
        "\n",
        "- It assumes that all features are independent of each other, which is rarely true in real-world data. This assumption is simple (na√Øve) but makes the computation very fast and effective.\n",
        "\n",
        "- The classifier assumes conditional independence between features $x_i$ given the class $y$, i.e., $P(x‚à£y) = \\prod_{i} P(x_i‚à£y)$. This assumption is often false in practice (features are frequently correlated), hence the term ‚Äúna√Øve‚Äù.\n",
        "\n",
        "###Despite the naive assumption, why it works well:\n",
        "\n",
        "-  Works well in many practical tasks (especially text classification / spam detection) because it estimates class-conditional probability distributions robustly and requires relatively few parameters.\n",
        "-  Extremely fast to train and predict; works well with high-dimensional data.\n",
        "-  Performs well with small training datasets and noisy data.\n",
        "\n",
        "###Limitations\n",
        "\n",
        "-  If feature dependence is strong and critical to class discrimination, performance may suffer.\n",
        "-  Requires appropriate probability models for features (Gaussian for continuous, Multinomial for counts, Bernoulli for binary features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43a4d0f3"
      },
      "source": [
        "##Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "Answer:\n",
        "\n",
        "###Overview / Intuition\n",
        "Na√Øve Bayes variants differ primarily in the assumed distribution for the feature likelihood $P(x_i‚à£y)$. Choose the variant that matches the nature of your features.\n",
        "\n",
        "###Gaussian Na√Øve Bayes\n",
        "\n",
        "- **Use when:** Features are continuous and roughly normally distributed within each class.\n",
        "\n",
        "- **Model:** For each feature and class, model $P(x_i‚à£y)$ as a Gaussian:\n",
        "\n",
        "$P(x_i‚à£y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i,y}}} \\exp\\left(-\\frac{(x_i - \\mu_{i,y})^2}{2\\sigma^2_{i,y}}\\right)$\n",
        "\n",
        "where $\\mu_{i,y}$ and $\\sigma^2_{i,y}$ are estimated from training data.\n",
        "\n",
        "- **Typical use:** Numeric features like height, weight, lab measurements.\n",
        "\n",
        "###Multinomial Na√Øve Bayes\n",
        "\n",
        "- **Use when:** Features are discrete counts (e.g., word counts in text classification ‚Äî bag-of-words).\n",
        "\n",
        "- **Model:** Class-conditional probability of counts; each class has a probability distribution over features (word probabilities). The likelihood of the feature vector is multinomial.\n",
        "\n",
        "- **Smoothing:** Usually uses Laplace (add-one) smoothing to handle zero counts.\n",
        "\n",
        "- **Typical use:** Document classification, where $x_i$ is count of word $i$.\n",
        "\n",
        "###Bernoulli Na√Øve Bayes\n",
        "\n",
        "- **Use when:** Features are binary (0/1), representing presence/absence of a feature (e.g., whether a word appears in a document).\n",
        "\n",
        "- **Model:** Each feature modeled as Bernoulli trial with probability $P(x_i=1‚à£y)$.\n",
        "\n",
        "- **Typical use:** Binary feature vectors, e.g., ‚Äúterm present or not‚Äù in text.\n",
        "\n",
        "###Choice Summary:\n",
        "\n",
        "- If features are real-valued and roughly normal ‚Üí GaussianNB.\n",
        "\n",
        "- If features are counts (frequency of terms) ‚Üí MultinomialNB.\n",
        "\n",
        "- If features are binary (present/absent) ‚Üí BernoulliNB."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "vu8iYI8ffFEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gnb_breast_cancer.py\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Scaling (not strictly required for GaussianNB, but sometimes helps)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Predict & evaluate\n",
        "y_pred = gnb.predict(X_test_scaled)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {acc:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=target_names))\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "\"\"\"Interpretation\n",
        "\n",
        "classification_report shows per-class precision, recall (sensitivity), and F1-score ‚Äî important for medical datasets where sensitivity (recall for the positive class) often matters\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ZdmJWmaThozQ",
        "outputId": "53dfd942-ef76-4f33-efc2-c3524eee6475"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9298\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.90      0.90      0.90        42\n",
            "      benign       0.94      0.94      0.94        72\n",
            "\n",
            "    accuracy                           0.93       114\n",
            "   macro avg       0.92      0.92      0.92       114\n",
            "weighted avg       0.93      0.93      0.93       114\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[38  4]\n",
            " [ 4 68]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Interpretation\\n\\nclassification_report shows per-class precision, recall (sensitivity), and F1-score ‚Äî important for medical datasets \\nwhere sensitivity (recall for the positive class) often matters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}